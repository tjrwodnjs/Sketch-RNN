{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dezero\n",
    "import dezero.functions as F\n",
    "import dezero.layers as L\n",
    "import dezero.models as M\n",
    "from dezero.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero import cuda\n",
    "from typing import List, Optional, Tuple, Any\n",
    "import math\n",
    "\n",
    "class StrokesDataset(dezero.DataLoader):\n",
    "    def __init__(self, data, batch_size, max_seq_length: int, scale: Optional[float] = None, shuffle=True, gpu=False):\n",
    "        stroke_data = []\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.data_size = len(data)\n",
    "        self.max_iter = math.ceil(self.data_size / batch_size)\n",
    "        self.gpu = gpu\n",
    "        \n",
    "        xp = cuda.cupy if self.gpu else np\n",
    "        \n",
    "        for seq in data:\n",
    "            # we will deem a sequence that is less than 10 as too short and thus ignore it\n",
    "            if 10 < len(seq) <= max_seq_length:\n",
    "                # clamp the delta x and delta y to [-1000, 1000]\n",
    "                seq = np.minimum(seq, 1000)\n",
    "                seq = np.maximum(seq, -1000)\n",
    "                \n",
    "                seq = np.array(seq, dtype=np.float32)\n",
    "                stroke_data.append(seq)\n",
    "        \n",
    "        if scale is None:\n",
    "            # calculate the scale factor\n",
    "            # the scale factor is the standard deviation of the x and y coordinates\n",
    "            # mean is not adjusted for simplicity\n",
    "            # 0:2 means the first two columns of the array which are x and y coordinates\n",
    "            scale = np.std(np.concatenate([np.ravel(s[:,0:2]) for s in stroke_data]))\n",
    "        \n",
    "        longest_seq_len = max([len(seq) for seq in stroke_data])\n",
    "        \n",
    "        # we add two extra columns to the dataset since we currently there are only 3 columns in the dataset\n",
    "        # additional two columns are for changing the last point 1/0 to a one-hot vector\n",
    "        temp_stroke_dataset = xp.zeros((len(stroke_data), longest_seq_len + 2, 5), dtype=np.float32)\n",
    "        \n",
    "        # self.mask is used to mark areas of the sequence that are not used\n",
    "        # we first initialize it to zero\n",
    "        temp_mask_dataset = xp.zeros((len(stroke_data), longest_seq_len + 1))\n",
    "        \n",
    "        self.dataset = []\n",
    "        \n",
    "        # start of sequence is [0, 0, 1, 0, 0]\n",
    "        \n",
    "        for i, seq in enumerate(stroke_data):\n",
    "            seq = xp.array(seq, dtype=xp.float32)\n",
    "            len_seq = len(seq)\n",
    "            \n",
    "            # we start from 1 to leave the first row for the start of sequence token\n",
    "            temp_stroke_dataset[i, 1:len_seq + 1, 0:2] = seq[:, :2] / scale # this is the x and y coordinates\n",
    "            temp_stroke_dataset[i, 1:len_seq + 1, 2] = 1 - seq[:, 2] # this is the pen down\n",
    "            temp_stroke_dataset[i, 1:len_seq + 1, 3] = seq[:, 2] # this is the pen up\n",
    "            temp_stroke_dataset[i, len_seq + 1, 4] = 1  # this is the end of sequence token\n",
    "            temp_mask_dataset[i, :len_seq + 1] = 1 # mask is on until the end of the sequence \n",
    "            # self.mask is used to mark areas of the sequence that are not used\n",
    "            # for example, if the sequence is shorter than the longest sequence, we use mask to ignore the rest of the sequence\n",
    "            # an example of mask is [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        \n",
    "        temp_stroke_dataset[:, 0, 2] = 1\n",
    "        \n",
    "        for i in range(len(stroke_data)):\n",
    "            self.dataset.append([temp_stroke_dataset[i], temp_mask_dataset[i]])\n",
    "        \n",
    "        \n",
    "        self.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dezero.functions as F\n",
    "\n",
    "# According to other estimates\n",
    "# the number of distributions in the mixture model is 20\n",
    "# https://github.com/Shun14/sketch-rnn-kanji\n",
    "# https://nn.labml.ai/sketch_rnn/index.html\n",
    "\n",
    "# This is for getting the loss of delta_x and delta_y\n",
    "class BivariateGaussianMixture:\n",
    "    def __init__(self, pi_logits, mu_x, mu_y, sigma_x, sigma_y, rho_xy):\n",
    "        # check if the pi_probs sum for each sequence is 1\n",
    "        # print('test pi', test_pi.shape) # pi shape is (batch_size, seq_len, n_distributions)\n",
    "        # check if the pi probabilities sum to 1\n",
    "        # seq_len = pi_logits.shape[1]\n",
    "        # print(F.reshape(F.sum(pi_logits, axis=2), (-1, seq_len)), 'sum of pi')\n",
    "        self.pi_logits = pi_logits\n",
    "        self.mu_x = mu_x\n",
    "        self.mu_y = mu_y\n",
    "        self.sigma_x = sigma_x\n",
    "        self.sigma_y = sigma_y\n",
    "        self.rho_xy = rho_xy\n",
    "    \n",
    "    @property\n",
    "    def n_distributions(self):\n",
    "        return self.pi_logits.shape[-1]\n",
    "    \n",
    "    def set_temperature(self, temperature: float):\n",
    "        self.pi_logits /= temperature\n",
    "        self.pi_logits = F.softmax(self.pi_logits, axis=2) # we do this to make sure the pi probabilities sum to 1\n",
    "        self.sigma_x *= math.sqrt(temperature)\n",
    "        self.sigma_y *= math.sqrt(temperature)\n",
    "    \n",
    "    def gaussian_pdf(self, x_delta, y_delta):\n",
    "        # the result means the probability of y in the normal distribution\n",
    "        # we check the probability of y in the normal distribution\n",
    "        # if the probability is high, the result is close to 1\n",
    "        # x_delta and y_delta shape are (batch_size, seq_len, hidden_size)\n",
    "        # mu_x and mu_y shape are (batch_size, seq_len, n_distributions)\n",
    "        norm1 = F.sub(x_delta, self.mu_x)\n",
    "        norm2 = F.sub(y_delta, self.mu_y)\n",
    "        xp = cuda.get_array_module(norm1)\n",
    "\n",
    "        dtype = self.sigma_x.dtype\n",
    "        max_dtype = xp.finfo(dtype).max\n",
    "        self.sigma_x = F.clip(self.sigma_x, 1e-5, max_dtype)\n",
    "        self.sigma_y = F.clip(self.sigma_y, 1e-5, max_dtype)\n",
    "        self.rho_xy = F.clip(self.rho_xy, -1 + 1e-5, 1 - 1e-5)\n",
    "        \n",
    "        s1s2 = F.mul(self.sigma_x, self.sigma_y)\n",
    "        \n",
    "        # This is from: https://github.com/hardmaru/write-rnn-tensorflow/blob/master/model.py\n",
    "        # z = tf.square(tf.div(norm1, s1)) + tf.square(tf.div(norm2, s2))\n",
    "        #     - 2 * tf.div(tf.multiply(rho, tf.multiply(norm1, norm2)), s1s2)\n",
    "         \n",
    "        # below is the deconstruction of the above linez\n",
    "        z_first_term = F.pow(F.div(norm1, self.sigma_x), 2)\n",
    "        z_second_term = F.pow(F.div(norm2, self.sigma_y), 2)\n",
    "        z_last_term_inner = F.mul(self.rho_xy, F.mul(norm1, norm2))\n",
    "        z_last_term_middle = F.div(z_last_term_inner, s1s2)\n",
    "        tmp_z = xp.ones(z_last_term_middle.shape) * -2\n",
    "        z_last_term = F.mul(tmp_z, z_last_term_middle)\n",
    "        z = F.add(F.add(z_first_term, z_second_term), z_last_term)\n",
    "        negRho = F.sub(np.ones(self.rho_xy.shape), F.pow(self.rho_xy, 2))\n",
    "\n",
    "        \n",
    "        result = F.exp(F.div(-z, 2 * negRho))\n",
    "        deno_first_term = np.ones(self.sigma_x.shape) * 2 * math.pi\n",
    "        denom_second_term = F.mul(s1s2, F.pow(negRho, 0.5))\n",
    "        denom = F.mul(deno_first_term, denom_second_term)\n",
    "        result = F.div(result, denom)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    # x1_data and x2_data are the real x and y coordinates of the stroke\n",
    "    def get_lossfunc(self, x_delta, y_delta, mask):\n",
    "        result0 = self.gaussian_pdf(x_delta, y_delta)\n",
    "        # check if result0 has inf or nan\n",
    "        # result0 shape is (batch_size, seq_len, n_distributions) 3D\n",
    "        result1 = F.mul(result0, self.pi_logits) # pi_logits shape is (batch_size, seq_len, n_distributions)\n",
    "        \n",
    "        result1 = F.sum(result1, axis=2, keepdims=True) # sum over the distributions\n",
    "        # the result1 shape is (batch_size, seq_len, 1)\n",
    "        # we reshape it to (batch_size, seq_len)\n",
    "        result1 = F.reshape(result1, result1.shape[:-1]) # result.shape[:-1] is (batch_size, seq_len)\n",
    "        \n",
    "        dtype = result1.dtype\n",
    "        max_dtype = np.finfo(dtype).max\n",
    "        result1 = F.clip(result1, 1e-5, max_dtype) \n",
    "\n",
    "        result1 = -F.log(result1) # result1 shape is (batch_size, seq_len)\n",
    "        # mask shape is also (batch_size, seq_len)\n",
    "        result1 = F.mul(result1, mask) # we multiply the mask to ignore the padding\n",
    "        \n",
    "        \n",
    "        # make the value to be one number\n",
    "        \n",
    "        \n",
    "        return F.mean(result1)\n",
    "    \n",
    "    def get_pi_idx(self, x, out_pi_elem):\n",
    "        # pdf shape is (batch_size, seq_len, n_distributions)\n",
    "        # let us only get the first batch\n",
    "        pdf = out_pi_elem\n",
    "        N = pdf.size\n",
    "        accumulate = 0\n",
    "        # print(pdf.size, 'pdf size')\n",
    "        # print(F.sum(pdf), 'sum of pdf')\n",
    "        # print(out_pi_elem.shape, 'out_pi_elem shape')\n",
    "        # print(x, 'x in pi idx', pdf, \" pdf\", pdf.shape)\n",
    "        # print(\"hello\",pdf[0], x)\n",
    "        for i in range(0, N):\n",
    "            # print(pdf[i].data, 'pdf[i].data')\n",
    "            accumulate += pdf[i].data\n",
    "            if accumulate >= x:\n",
    "                return i\n",
    "            # print(accumulate, 'accumulate')\n",
    "        print('error with sampling ensemble')\n",
    "        return -1\n",
    "    \n",
    "    # M means the number of samples\n",
    "    def sample(self, count, M=15):\n",
    "        xp = cuda.get_array_module(self.pi_logits)\n",
    "        # get the index of the distribution\n",
    "        \n",
    "        result = xp.random.rand(count, M, 3) # initially random [0,1]\n",
    "        # print(result.shape, 'result shape')\n",
    "        # we will get result for delta_x and delta_y\n",
    "        rn = xp.random.rand(count, M, 2) \n",
    "        mu = 0\n",
    "        std = 0\n",
    "        idx = 0\n",
    "        \n",
    "        # currently the pi logits shape is (batch_size, seq_len, n_distributions)\n",
    "        # we will only get the first batch for now'\n",
    "        # print(self.pi_logits.shape, 'pi logits shape')\n",
    "        # print(self.pi_logits[0].shape, 'pi logits shape')\n",
    "        # print(\"get sum of pi\", F.sum(self.pi_logits[0]))\n",
    "        out_pi = self.pi_logits[0] # out_pi shape is (seq_len, n_distributions)\n",
    "        # print(out_pi.shape, 'out_pi shape')\n",
    "        # print(\"mu_x\", self.mu_x.shape)\n",
    "        # print(\"std\", self.sigma_x.shape)\n",
    "        \n",
    "        # we do not need to get batch size since we are only getting the first batch\n",
    "        mu_x = self.mu_x[0]\n",
    "        mu_y = self.mu_y[0]\n",
    "        sigma_x = self.sigma_x[0]\n",
    "        sigma_y = self.sigma_y[0]\n",
    "        rho_xy = self.rho_xy[0]\n",
    "        \n",
    "        for j in range(M):\n",
    "            for i in range(count):\n",
    "                # we only get the first element since we only need one\n",
    "                idx = self.get_pi_idx(result[i, j, 0], out_pi[i])\n",
    "                mu = [mu_x[i, idx], mu_y[i, idx]]\n",
    "                std = [sigma_x[i, idx], sigma_y[i, idx]]\n",
    "                rho = rho_xy[i, idx]\n",
    "                \n",
    "            \n",
    "                # print(mu + rn[i, j] * std, 'mu + rn[i, j] * std')\n",
    "                result_x_y = (mu + rn[i, j] * std)\n",
    "                # print(result_x_y[0].data, 'this is resuult')\n",
    "                result[i, j, 0] = result_x_y[0].data\n",
    "                result[i, j, 1] = result_x_y[1].data\n",
    "        return result\n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n",
      "(4, 64, 5) (4, 63)\n",
      "[[ 0.          0.          1.          0.          0.        ]\n",
      " [-0.5628141  -0.5959208   1.          0.          0.        ]\n",
      " [-0.62902755 -0.46349397  1.          0.          0.        ]\n",
      " [-1.0594149  -0.06621343  1.          0.          0.        ]\n",
      " [-0.43038726  0.16553356  1.          0.          0.        ]\n",
      " [-0.5628141   0.36417383  1.          0.          0.        ]\n",
      " [-0.2979604   0.36417383  1.          0.          0.        ]\n",
      " [-0.2979604   1.5891222   1.          0.          0.        ]\n",
      " [ 0.09932014  1.0594149   1.          0.          0.        ]\n",
      " [ 0.2979604   0.4966007   1.          0.          0.        ]\n",
      " [ 2.1188297   2.3174698   1.          0.          0.        ]\n",
      " [ 0.2979604   1.4898021   1.          0.          0.        ]\n",
      " [ 0.33106712  0.33106712  1.          0.          0.        ]\n",
      " [ 0.62902755  0.09932014  1.          0.          0.        ]\n",
      " [ 0.39728054 -0.4966007   1.          0.          0.        ]\n",
      " [ 0.72834766  0.          1.          0.          0.        ]\n",
      " [ 0.33106712  0.19864027  1.          0.          0.        ]\n",
      " [ 0.16553356 -0.06621343  1.          0.          0.        ]\n",
      " [ 0.4966007  -1.1256282   1.          0.          0.        ]\n",
      " [ 1.2249484  -1.2249484   1.          0.          0.        ]\n",
      " [ 0.16553356 -0.33106712  1.          0.          0.        ]\n",
      " [ 0.19864027 -0.4966007   1.          0.          0.        ]\n",
      " [ 0.19864027 -1.0925215   1.          0.          0.        ]\n",
      " [ 0.06621343 -1.2249484   1.          0.          0.        ]\n",
      " [-0.16553356 -0.43038726  1.          0.          0.        ]\n",
      " [-0.39728054 -0.4966007   1.          0.          0.        ]\n",
      " [-1.5891222  -1.0925215   1.          0.          0.        ]\n",
      " [-0.43038726 -0.03310671  1.          0.          0.        ]\n",
      " [-0.46349397  0.19864027  1.          0.          0.        ]\n",
      " [-0.5959208   0.4966007   1.          0.          0.        ]\n",
      " [-0.16553356  0.26485372  0.          1.          0.        ]\n",
      " [ 0.09932014 -0.62902755  1.          0.          0.        ]\n",
      " [-0.03310671 -0.52970743  1.          0.          0.        ]\n",
      " [ 0.89388126 -1.7877625   0.          1.          0.        ]\n",
      " [-0.5959208   0.92698795  1.          0.          0.        ]\n",
      " [ 0.13242686 -0.16553356  1.          0.          0.        ]\n",
      " [ 0.9932014  -0.2979604   1.          0.          0.        ]\n",
      " [ 0.62902755 -0.03310671  1.          0.          0.        ]\n",
      " [ 1.0263081   0.5959208   1.          0.          0.        ]\n",
      " [-0.26485372  0.          1.          0.          0.        ]\n",
      " [-0.62902755  0.23174699  1.          0.          0.        ]\n",
      " [-0.82766783  0.52970743  1.          0.          0.        ]\n",
      " [-0.72834766  0.03310671  1.          0.          0.        ]\n",
      " [-0.5959208  -0.26485372  0.          1.          0.        ]\n",
      " [ 0.          0.          0.          0.          1.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "44 44\n",
      "27 27\n",
      "34 34\n",
      "49 49\n"
     ]
    }
   ],
   "source": [
    "data = np.load('./data/sketchrnn_apple.npz', encoding='latin1', allow_pickle=True)\n",
    "\n",
    "strokes = StrokesDataset(data['train'], batch_size=4, max_seq_length=200, gpu=False, shuffle=False)\n",
    "print(strokes.data_size)\n",
    "# first item\n",
    "\n",
    "x, t = strokes.__next__()\n",
    "print(x.shape, t.shape) # x is the stroke, t is the mask (x has one more column than t)\n",
    "print(x[0], t[0])\n",
    "\n",
    "# check if the mask is working\n",
    "batch_size = x.shape[0]\n",
    "\n",
    "for i in range(batch_size):\n",
    "    mask_zero_id = np.where(t[i] == 0)[0]\n",
    "    # first id\n",
    "    first_id = mask_zero_id[0]\n",
    "    stroke_end_id = np.where(x[i, :, 4] == 1)[0]\n",
    "    first_stroke_end_id = stroke_end_id[0]\n",
    "    \n",
    "    print(first_id, first_stroke_end_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, d_z, hidden_size):\n",
    "        super().__init__()\n",
    "        self.lstm = L.LSTM(in_size=5, hidden_size=hidden_size)\n",
    "        self.mu_head = L.Linear(in_size=hidden_size, out_size=d_z)\n",
    "        self.sigma_head = L.Linear(in_size=hidden_size, out_size=d_z)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.d_z = d_z\n",
    "\n",
    "    def forward(self, x):\n",
    "        # hidden = self.lstm(x)\n",
    "\n",
    "        # hidden = hidden[:,-1,:]        \n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        h, c = None, None\n",
    "        for i in range(seq_len):\n",
    "            if h is None or c is None:\n",
    "                h, c = self.lstm(x[:, i, :])\n",
    "            else:\n",
    "                h, c = self.lstm(x[:, i, :], h, c)\n",
    "\n",
    "        mu = self.mu_head(h)\n",
    "        sigma_hat = self.sigma_head(h)\n",
    "        sigma = F.exp(sigma_hat / 2.)\n",
    "\n",
    "        xp = dezero.cuda.get_array_module(mu)\n",
    "        z = mu + sigma * xp.random.normal(0, 1, mu.shape)\n",
    "        return z, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, d_z, hidden_size, n_distributions):\n",
    "        super().__init__()\n",
    "        self.lstm = L.LSTM(in_size=d_z+5, hidden_size=hidden_size)\n",
    "\n",
    "        self.init_h = L.Linear(in_size=d_z, out_size=hidden_size)\n",
    "        self.init_c = L.Linear(in_size=d_z, out_size=hidden_size)\n",
    "\n",
    "        self.pi_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.mu_x_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.mu_y_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.sigma_x_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.sigma_y_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.rho_xy_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "\n",
    "        self.q_head = L.Linear(in_size=hidden_size, out_size=3)\n",
    "\n",
    "        self.n_distributions = n_distributions\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x, z, h=None, c=None):\n",
    "        xp = dezero.cuda.get_array_module(x)\n",
    "        h, c = None, None\n",
    "        if h is None and c is None:\n",
    "            h = F.tanh(self.init_h(z))\n",
    "            c = F.tanh(self.init_c(z))\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        outputs = None\n",
    "        for i in range(seq_len):\n",
    "            h, c = self.lstm(x[:, i, :], h, c)\n",
    "            if outputs == None:\n",
    "                outputs = F.expand_dims(h, 1)\n",
    "            else:\n",
    "                outputs = F.cat((outputs, F.expand_dims(h, 1)), axis=1)\n",
    "\n",
    "        # hidden Needs to chagned to output of lstm\n",
    "        print(outputs.shape)\n",
    "        print(self.q_head(outputs).shape)\n",
    "\n",
    "        outputs= F.reshape(outputs, (-1, self.hidden_size))\n",
    "        q_logits = F.log_softmax(self.q_head(outputs))\n",
    "        print(q_logits.shape, \"q_logits\")\n",
    "\n",
    "        pi_logits = self.pi_head(outputs)\n",
    "        mu_x = self.mu_x_head(outputs)\n",
    "        mu_y = self.mu_y_head(outputs)\n",
    "        sigma_x = self.sigma_x_head(outputs)\n",
    "        sigma_y = self.sigma_y_head(outputs)\n",
    "        rho_xy = self.rho_xy_head(outputs)\n",
    "\n",
    "        pi_logits = F.reshape(pi_logits, (-1, seq_len, self.n_distributions))\n",
    "        mu_x = F.reshape(mu_x, (-1, seq_len, self.n_distributions))\n",
    "        mu_y = F.reshape(mu_y, (-1, seq_len, self.n_distributions))\n",
    "        sigma_x = F.reshape(sigma_x, (-1, seq_len, self.n_distributions))\n",
    "        sigma_y = F.reshape(sigma_y, (-1, seq_len, self.n_distributions))\n",
    "        rho_xy = F.reshape(rho_xy, (-1, seq_len, self.n_distributions))\n",
    "        \n",
    "        q_logits = F.reshape(q_logits, (-1, seq_len, 3))\n",
    "\n",
    "        pi_probs = F.softmax(pi_logits, axis=2)\n",
    "\n",
    "        bgm = BivariateGaussianMixture(pi_probs, mu_x, mu_y, F.exp(sigma_x), F.exp(sigma_y), F.tanh(rho_xy))\n",
    "        return bgm, q_logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ReconstructionLoss(target, mask, bgm, q_logits):\n",
    "#     xp = cuda.get_array_module(mask)\n",
    "#     xy = target[:, :, 0:2]\n",
    "#     xy = xy[:, :, xp.newaxis, :]\n",
    "#     xy = xp.tile(xy, (1, 1, bgm.n_distributions, 1))\n",
    "    \n",
    "#     # expanded_shape = (xy.shape[0], xy.shape[1], bgm.n_distributions, xy.shape[3])\n",
    "    \n",
    "#     x = xy[:,:,:,0]\n",
    "#     y = xy[:,:,:,0]\n",
    "    \n",
    "#     loss_stroke = F.mul(bgm.get_lossfunc(x, y), mask)\n",
    "    \n",
    "#     loss_pen = -F.mean(F.mul(target[:,:,2:], q_logits))\n",
    "    \n",
    "#     return F.add(loss_stroke, loss_pen)\n",
    "\n",
    "\n",
    "def ReconstructionLoss(mask, target, bgm, q_logits):\n",
    "        xp = cuda.get_array_module(mask)\n",
    "        # target is a 3 dimensional array\n",
    "        # xy = target[:, :, 0:2].unsqueeze(-2).expand(-1, -1, dist.n_distributions, -1)\n",
    "        xy = target[:, :, 0:2]\n",
    "        x = xy[:, :, 0]\n",
    "        y = xy[:, :, 1]\n",
    "        \n",
    "        distributions = bgm.n_distributions\n",
    "        stacked_x = None\n",
    "        stacked_y = None\n",
    "        for i in range(distributions):\n",
    "            if stacked_x is None:\n",
    "                stacked_x = F.expand_dims(x, axis=2)\n",
    "                stacked_y = F.expand_dims(y, axis=2)\n",
    "            else:\n",
    "                stacked_x = F.cat((stacked_x, F.expand_dims(x, axis=2)), axis=2)\n",
    "                stacked_y = F.cat((stacked_y, F.expand_dims(y, axis=2)), axis=2)\n",
    "        \n",
    "        # expanded_shape = (xy.shape[0], xy.shape[1], bgm.n_distributions, xy.shape[3])\n",
    "        \n",
    "        \n",
    "        # x = xp.tile(xy, expanded_shape)\n",
    "        # y = xp.tile(xy, expanded_shape)\n",
    "        # loss_stroke 에 문제\n",
    "        \n",
    "        loss_stroke = bgm.get_lossfunc(stacked_x, stacked_y, mask)\n",
    "        \n",
    "        loss_pen = -F.mean(F.mul(target[:,:,2:], q_logits))\n",
    "        \n",
    "        return F.add(loss_stroke, loss_pen)\n",
    "        \n",
    "        \n",
    "\n",
    "def KLDivergenceLoss(mu, sigma):\n",
    "    xp = cuda.get_array_module(mu)\n",
    "    tmp = xp.ones(sigma.shape)\n",
    "    inner_1 = F.add(tmp, sigma)\n",
    "    inner_2 = F.add(F.pow(mu, 2), F.exp(sigma))\n",
    "    inner = F.sub(inner_1, inner_2)\n",
    "    tmp2 = xp.ones(inner.shape) * -2\n",
    "    return F.mean(F.div(inner, tmp2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, d_z, enc_hidden_size, dec_hidden_size, n_distributions):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_z, enc_hidden_size)\n",
    "        self.decoder = Decoder(d_z, dec_hidden_size, n_distributions)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        z, mu, sigma = self.encoder(x)\n",
    "        \n",
    "        xp = cuda.get_array_module(z)\n",
    "        z = z.data\n",
    "        z_stack = xp.tile(z[:, xp.newaxis, :], (1, x.shape[1]-1, 1))\n",
    "\n",
    "        inputs = F.cat((x[:,:-1], z_stack), axis=2)\n",
    "        # inputs = dezero.as_variable(inputs)\n",
    "        bgm, q_logits = self.decoder(inputs, z)\n",
    "\n",
    "        kl_loss = KLDivergenceLoss(mu, sigma)\n",
    "\n",
    "        print(x.shape, t.shape)\n",
    "        rec_loss = ReconstructionLoss(t, x[:,1:], bgm, q_logits)\n",
    "        # \n",
    "        return kl_loss + rec_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_z = 4\n",
    "enc_hidden_size = 32\n",
    "dec_hidden_size = 64\n",
    "n_distributions = 8\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 63, 64)\n",
      "(4, 63, 3)\n",
      "(252, 3) q_logits\n",
      "(4, 64, 5) (4, 63)\n",
      "1.9921323212335484\n",
      "(4, 63, 64)\n",
      "(4, 63, 3)\n",
      "(252, 3) q_logits\n",
      "(4, 64, 5) (4, 63)\n",
      "2.1741877686748308\n",
      "(4, 63, 64)\n",
      "(4, 63, 3)\n",
      "(252, 3) q_logits\n",
      "(4, 64, 5) (4, 63)\n",
      "1.9985753213135138\n",
      "(4, 63, 64)\n",
      "(4, 63, 3)\n",
      "(252, 3) q_logits\n",
      "(4, 64, 5) (4, 63)\n",
      "1.8726277028297136\n",
      "(4, 63, 64)\n",
      "(4, 63, 3)\n",
      "(252, 3) q_logits\n",
      "(4, 64, 5) (4, 63)\n",
      "1.9341660210065472\n",
      "(4, 63, 64)\n",
      "(4, 63, 3)\n",
      "(252, 3) q_logits\n",
      "(4, 64, 5) (4, 63)\n",
      "2.3542260072547174\n",
      "(4, 63, 64)\n",
      "(4, 63, 3)\n",
      "(252, 3) q_logits\n",
      "(4, 64, 5) (4, 63)\n",
      "1.8502045099928224\n",
      "(4, 63, 64)\n",
      "(4, 63, 3)\n",
      "(252, 3) q_logits\n",
      "(4, 64, 5) (4, 63)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m model(x, t)\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mcleargrads()\n\u001b[1;32m---> 11\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\흠\\Sketch-RNN\\dezero\\core.py:130\u001b[0m, in \u001b[0;36mVariable.backward\u001b[1;34m(self, retain_grad, create_graph)\u001b[0m\n\u001b[0;32m    127\u001b[0m gys \u001b[38;5;241m=\u001b[39m [output()\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39moutputs]  \u001b[38;5;66;03m# output is weakref\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m using_config(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_backprop\u001b[39m\u001b[38;5;124m\"\u001b[39m, create_graph):\n\u001b[1;32m--> 130\u001b[0m     gxs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gxs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    132\u001b[0m         gxs \u001b[38;5;241m=\u001b[39m (gxs,)\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\흠\\Sketch-RNN\\dezero\\functions.py:384\u001b[0m, in \u001b[0;36mLinear.backward\u001b[1;34m(self, gy)\u001b[0m\n\u001b[0;32m    382\u001b[0m x, W, b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs\n\u001b[0;32m    383\u001b[0m gb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m b\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sum_to(gy, b\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 384\u001b[0m gx \u001b[38;5;241m=\u001b[39m \u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m gW \u001b[38;5;241m=\u001b[39m matmul(x\u001b[38;5;241m.\u001b[39mT, gy)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gx, gW, gb\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\흠\\Sketch-RNN\\dezero\\functions.py:371\u001b[0m, in \u001b[0;36mmatmul\u001b[1;34m(x, W)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatmul\u001b[39m(x, W):\n\u001b[1;32m--> 371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\흠\\Sketch-RNN\\dezero\\core.py:207\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m    204\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [as_variable(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m    206\u001b[0m xs \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m--> 207\u001b[0m ys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ys, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    209\u001b[0m     ys \u001b[38;5;241m=\u001b[39m (ys,)\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\흠\\Sketch-RNN\\dezero\\functions.py:360\u001b[0m, in \u001b[0;36mMatMul.forward\u001b[1;34m(self, x, W)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, W):\n\u001b[1;32m--> 360\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dezero.optimizers import Adam\n",
    "\n",
    "model = VAE(d_z, enc_hidden_size, dec_hidden_size, n_distributions)\n",
    "optimizer = Adam().setup(model)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(strokes.max_iter):\n",
    "        x, t = strokes.__next__()\n",
    "        loss = model(x, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        print(loss.data)\n",
    "        \n",
    "    print(f\"Epoch {epoch} finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
