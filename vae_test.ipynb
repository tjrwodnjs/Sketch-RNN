{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package           Version\n",
      "----------------- -----------\n",
      "appnope           0.1.4\n",
      "asttokens         2.4.1\n",
      "comm              0.2.2\n",
      "contourpy         1.2.0\n",
      "cycler            0.12.1\n",
      "debugpy           1.8.1\n",
      "decorator         5.1.1\n",
      "exceptiongroup    1.2.0\n",
      "executing         2.0.1\n",
      "fonttools         4.50.0\n",
      "ipykernel         6.29.4\n",
      "ipython           8.22.2\n",
      "jedi              0.19.1\n",
      "jupyter_client    8.6.1\n",
      "jupyter_core      5.7.2\n",
      "kiwisolver        1.4.5\n",
      "matplotlib        3.8.3\n",
      "matplotlib-inline 0.1.6\n",
      "nest-asyncio      1.6.0\n",
      "numpy             1.26.4\n",
      "packaging         24.0\n",
      "parso             0.8.3\n",
      "pexpect           4.9.0\n",
      "pillow            10.2.0\n",
      "pip               23.0.1\n",
      "platformdirs      4.2.0\n",
      "prompt-toolkit    3.0.43\n",
      "psutil            5.9.8\n",
      "ptyprocess        0.7.0\n",
      "pure-eval         0.2.2\n",
      "Pygments          2.17.2\n",
      "pyparsing         3.1.2\n",
      "python-dateutil   2.9.0.post0\n",
      "pyzmq             25.1.2\n",
      "setuptools        65.5.0\n",
      "six               1.16.0\n",
      "stack-data        0.6.3\n",
      "tornado           6.4\n",
      "traitlets         5.14.2\n",
      "wcwidth           0.2.13\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dezero\n",
    "import dezero.functions as F\n",
    "import dezero.layers as L\n",
    "import dezero.models as M\n",
    "from dezero.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero import cuda\n",
    "from typing import List, Optional, Tuple, Any\n",
    "import math\n",
    "\n",
    "class StrokesDataset(dezero.DataLoader):\n",
    "    def __init__(self, data, batch_size, max_seq_length: int, scale: Optional[float] = None, shuffle=True, gpu=False):\n",
    "        stroke_data = []\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.data_size = len(data)\n",
    "        self.max_iter = math.ceil(self.data_size / batch_size)\n",
    "        self.gpu = gpu\n",
    "        \n",
    "        xp = cuda.cupy if self.gpu else np\n",
    "        \n",
    "        for seq in data:\n",
    "            # we will deem a sequence that is less than 10 as too short and thus ignore it\n",
    "            if 10 < len(seq) <= max_seq_length:\n",
    "                # clamp the delta x and delta y to [-1000, 1000]\n",
    "                seq = np.minimum(seq, 1000)\n",
    "                seq = np.maximum(seq, -1000)\n",
    "                \n",
    "                seq = np.array(seq, dtype=np.float32)\n",
    "                stroke_data.append(seq)\n",
    "        \n",
    "        if scale is None:\n",
    "            # calculate the scale factor\n",
    "            # the scale factor is the standard deviation of the x and y coordinates\n",
    "            # mean is not adjusted for simplicity\n",
    "            # 0:2 means the first two columns of the array which are x and y coordinates\n",
    "            scale = np.std(np.concatenate([np.ravel(s[:,0:2]) for s in stroke_data]))\n",
    "        \n",
    "        longest_seq_len = max([len(seq) for seq in stroke_data])\n",
    "        \n",
    "        # we add two extra columns to the dataset since we currently there are only 3 columns in the dataset\n",
    "        # additional two columns are for changing the last point 1/0 to a one-hot vector\n",
    "        temp_stroke_dataset = xp.zeros((len(stroke_data), longest_seq_len + 2, 5), dtype=np.float32)\n",
    "        \n",
    "        # self.mask is used to mark areas of the sequence that are not used\n",
    "        # we first initialize it to zero\n",
    "        temp_mask_dataset = xp.zeros((len(stroke_data), longest_seq_len + 1))\n",
    "        \n",
    "        self.dataset = []\n",
    "        \n",
    "        # start of sequence is [0, 0, 1, 0, 0]\n",
    "        \n",
    "        for i, seq in enumerate(stroke_data):\n",
    "            seq = xp.array(seq, dtype=xp.float32)\n",
    "            len_seq = len(seq)\n",
    "            \n",
    "            # we start from 1 to leave the first row for the start of sequence token\n",
    "            temp_stroke_dataset[i, 1:len_seq + 1, 0:2] = seq[:, :2] / scale # this is the x and y coordinates\n",
    "            temp_stroke_dataset[i, 1:len_seq + 1, 2] = 1 - seq[:, 2] # this is the pen down\n",
    "            temp_stroke_dataset[i, 1:len_seq + 1, 3] = seq[:, 2] # this is the pen up\n",
    "            temp_stroke_dataset[i, len_seq + 1, 4] = 1  # this is the end of sequence token\n",
    "            temp_mask_dataset[i, :len_seq + 1] = 1 # mask is on until the end of the sequence \n",
    "            # self.mask is used to mark areas of the sequence that are not used\n",
    "            # for example, if the sequence is shorter than the longest sequence, we use mask to ignore the rest of the sequence\n",
    "            # an example of mask is [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        \n",
    "        temp_stroke_dataset[:, 0, 2] = 1\n",
    "        \n",
    "        for i in range(len(stroke_data)):\n",
    "            self.dataset.append([temp_stroke_dataset[i], temp_mask_dataset[i]])\n",
    "        \n",
    "        \n",
    "        self.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dezero.functions as F\n",
    "\n",
    "# According to other estimates\n",
    "# the number of distributions in the mixture model is 20\n",
    "# https://github.com/Shun14/sketch-rnn-kanji\n",
    "# https://nn.labml.ai/sketch_rnn/index.html\n",
    "\n",
    "# This is for getting the loss of delta_x and delta_y\n",
    "class BivariateGaussianMixture:\n",
    "    def __init__(self, pi_logits, mu_x, mu_y, sigma_x, sigma_y, rho_xy):\n",
    "        # check if the pi_probs sum for each sequence is 1\n",
    "        # print('test pi', test_pi.shape) # pi shape is (batch_size, seq_len, n_distributions)\n",
    "        # check if the pi probabilities sum to 1\n",
    "        # seq_len = pi_logits.shape[1]\n",
    "        # print(F.reshape(F.sum(pi_logits, axis=2), (-1, seq_len)), 'sum of pi')\n",
    "        self.pi_logits = pi_logits\n",
    "        self.pi_probs = F.softmax(pi_logits, axis=2)\n",
    "        self.mu_x = mu_x\n",
    "        self.mu_y = mu_y\n",
    "        self.sigma_x = sigma_x\n",
    "        self.sigma_y = sigma_y\n",
    "        self.rho_xy = rho_xy\n",
    "    \n",
    "    @property\n",
    "    def n_distributions(self):\n",
    "        return self.pi_logits.shape[-1]\n",
    "    \n",
    "    def set_temperature(self, temperature: float):\n",
    "        self.pi_logits /= temperature\n",
    "        self.pi_probs = F.softmax(self.pi_logits, axis=2) # we do this to make sure the pi probabilities sum to 1\n",
    "        self.sigma_x *= math.sqrt(temperature)\n",
    "        self.sigma_y *= math.sqrt(temperature)\n",
    "    \n",
    "    def gaussian_pdf(self, x_delta, y_delta):\n",
    "        # the result means the probability of y in the normal distribution\n",
    "        # we check the probability of y in the normal distribution\n",
    "        # if the probability is high, the result is close to 1\n",
    "        # x_delta and y_delta shape are (batch_size, seq_len, hidden_size)\n",
    "        # mu_x and mu_y shape are (batch_size, seq_len, n_distributions)\n",
    "        norm1 = F.sub(x_delta, self.mu_x)\n",
    "        norm2 = F.sub(y_delta, self.mu_y)\n",
    "        xp = cuda.get_array_module(norm1)\n",
    "\n",
    "        dtype = self.sigma_x.dtype\n",
    "        max_dtype = xp.finfo(dtype).max\n",
    "        self.sigma_x = F.clip(self.sigma_x, 1e-5, max_dtype)\n",
    "        self.sigma_y = F.clip(self.sigma_y, 1e-5, max_dtype)\n",
    "        self.rho_xy = F.clip(self.rho_xy, -1 + 1e-5, 1 - 1e-5)\n",
    "        \n",
    "        s1s2 = F.mul(self.sigma_x, self.sigma_y)\n",
    "        \n",
    "        # This is from: https://github.com/hardmaru/write-rnn-tensorflow/blob/master/model.py\n",
    "        # z = tf.square(tf.div(norm1, s1)) + tf.square(tf.div(norm2, s2))\n",
    "        #     - 2 * tf.div(tf.multiply(rho, tf.multiply(norm1, norm2)), s1s2)\n",
    "         \n",
    "        # below is the deconstruction of the above linez\n",
    "        z_first_term = F.pow(F.div(norm1, self.sigma_x), 2)\n",
    "        z_second_term = F.pow(F.div(norm2, self.sigma_y), 2)\n",
    "        z_last_term_inner = F.mul(self.rho_xy, F.mul(norm1, norm2))\n",
    "        z_last_term_middle = F.div(z_last_term_inner, s1s2)\n",
    "        tmp_z = xp.ones(z_last_term_middle.shape) * -2\n",
    "        z_last_term = F.mul(tmp_z, z_last_term_middle)\n",
    "        z = F.add(F.add(z_first_term, z_second_term), z_last_term)\n",
    "        negRho = F.sub(np.ones(self.rho_xy.shape), F.pow(self.rho_xy, 2))\n",
    "\n",
    "        \n",
    "        result = F.exp(F.div(-z, 2 * negRho))\n",
    "        deno_first_term = np.ones(self.sigma_x.shape) * 2 * math.pi\n",
    "        denom_second_term = F.mul(s1s2, F.pow(negRho, 0.5))\n",
    "        denom = F.mul(deno_first_term, denom_second_term)\n",
    "        result = F.div(result, denom)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    # x1_data and x2_data are the real x and y coordinates of the stroke\n",
    "    def get_lossfunc(self, x_delta, y_delta, mask):\n",
    "        result0 = self.gaussian_pdf(x_delta, y_delta)\n",
    "        # check if result0 has inf or nan\n",
    "        # result0 shape is (batch_size, seq_len, n_distributions) 3D\n",
    "        result1 = F.mul(result0, self.pi_logits) # pi_logits shape is (batch_size, seq_len, n_distributions)\n",
    "        \n",
    "        result1 = F.sum(result1, axis=2, keepdims=True) # sum over the distributions\n",
    "        # the result1 shape is (batch_size, seq_len, 1)\n",
    "        # we reshape it to (batch_size, seq_len)\n",
    "        result1 = F.reshape(result1, result1.shape[:-1]) # result.shape[:-1] is (batch_size, seq_len)\n",
    "        \n",
    "        dtype = result1.dtype\n",
    "        max_dtype = np.finfo(dtype).max\n",
    "        result1 = F.clip(result1, 1e-5, max_dtype) \n",
    "\n",
    "        result1 = -F.log(result1) # result1 shape is (batch_size, seq_len)\n",
    "        # mask shape is also (batch_size, seq_len)\n",
    "        result1 = F.mul(result1, mask) # we multiply the mask to ignore the padding\n",
    "        \n",
    "        \n",
    "        # make the value to be one number\n",
    "        \n",
    "        \n",
    "        return F.mean(result1)\n",
    "    \n",
    "    def get_pi_idx(self, x, out_pi_elem):\n",
    "        # pdf shape is (batch_size, seq_len, n_distributions)\n",
    "        # let us only get the first batch\n",
    "        pdf = out_pi_elem\n",
    "        N = pdf.size\n",
    "        accumulate = 0\n",
    "        # print(pdf.size, 'pdf size')\n",
    "        # print(F.sum(pdf), 'sum of pdf')\n",
    "        # print(out_pi_elem.shape, 'out_pi_elem shape')\n",
    "        # print(x, 'x in pi idx', pdf, \" pdf\", pdf.shape)\n",
    "        # print(\"hello\",pdf[0], x)\n",
    "        for i in range(0, N):\n",
    "            # print(pdf[i].data, 'pdf[i].data')\n",
    "            accumulate += pdf[i].data\n",
    "            if accumulate >= x:\n",
    "                return i\n",
    "            # print(accumulate, 'accumulate')\n",
    "        print('error with sampling ensemble')\n",
    "        return -1\n",
    "    \n",
    "    # M means the number of samples\n",
    "    def sample(self, count, M=15):\n",
    "        xp = cuda.get_array_module(self.pi_logits)\n",
    "        # get the index of the distribution\n",
    "        \n",
    "        result = xp.random.rand(count, M, 3) # initially random [0,1]\n",
    "        # print(result.shape, 'result shape')\n",
    "        # we will get result for delta_x and delta_y\n",
    "        rn = xp.random.rand(count, M, 2) \n",
    "        mu = 0\n",
    "        std = 0\n",
    "        idx = 0\n",
    "        \n",
    "        # currently the pi logits shape is (batch_size, seq_len, n_distributions)\n",
    "        # we will only get the first batch for now'\n",
    "        # print(self.pi_logits.shape, 'pi logits shape')\n",
    "        # print(self.pi_logits[0].shape, 'pi logits shape')\n",
    "        # print(\"get sum of pi\", F.sum(self.pi_logits[0]))\n",
    "        out_pi = self.pi_probs[0] # out_pi shape is (seq_len, n_distributions)\n",
    "        # print(out_pi.shape, 'out_pi shape')\n",
    "        # print(\"mu_x\", self.mu_x.shape)\n",
    "        # print(\"std\", self.sigma_x.shape)\n",
    "        \n",
    "        # we do not need to get batch size since we are only getting the first batch\n",
    "        mu_x = self.mu_x[0]\n",
    "        mu_y = self.mu_y[0]\n",
    "        sigma_x = self.sigma_x[0]\n",
    "        sigma_y = self.sigma_y[0]\n",
    "        rho_xy = self.rho_xy[0]\n",
    "        \n",
    "        for j in range(M):\n",
    "            for i in range(count):\n",
    "                # we only get the first element since we only need one\n",
    "                idx = self.get_pi_idx(result[i, j, 0], out_pi[i])\n",
    "                mu = [mu_x[i, idx], mu_y[i, idx]]\n",
    "                std = [sigma_x[i, idx], sigma_y[i, idx]]\n",
    "                rho = rho_xy[i, idx]\n",
    "                \n",
    "            \n",
    "                # print(mu + rn[i, j] * std, 'mu + rn[i, j] * std')\n",
    "                result_x_y = (mu + rn[i, j] * std)\n",
    "                # print(result_x_y[0].data, 'this is resuult')\n",
    "                result[i, j, 0] = result_x_y[0].data\n",
    "                result[i, j, 1] = result_x_y[1].data\n",
    "        return result\n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n",
      "(4, 64, 5) (4, 63)\n",
      "[[ 0.          0.          1.          0.          0.        ]\n",
      " [-0.5628141  -0.5959208   1.          0.          0.        ]\n",
      " [-0.62902755 -0.46349397  1.          0.          0.        ]\n",
      " [-1.0594149  -0.06621343  1.          0.          0.        ]\n",
      " [-0.43038726  0.16553356  1.          0.          0.        ]\n",
      " [-0.5628141   0.36417383  1.          0.          0.        ]\n",
      " [-0.2979604   0.36417383  1.          0.          0.        ]\n",
      " [-0.2979604   1.5891222   1.          0.          0.        ]\n",
      " [ 0.09932014  1.0594149   1.          0.          0.        ]\n",
      " [ 0.2979604   0.4966007   1.          0.          0.        ]\n",
      " [ 2.1188297   2.3174698   1.          0.          0.        ]\n",
      " [ 0.2979604   1.4898021   1.          0.          0.        ]\n",
      " [ 0.33106712  0.33106712  1.          0.          0.        ]\n",
      " [ 0.62902755  0.09932014  1.          0.          0.        ]\n",
      " [ 0.39728054 -0.4966007   1.          0.          0.        ]\n",
      " [ 0.72834766  0.          1.          0.          0.        ]\n",
      " [ 0.33106712  0.19864027  1.          0.          0.        ]\n",
      " [ 0.16553356 -0.06621343  1.          0.          0.        ]\n",
      " [ 0.4966007  -1.1256282   1.          0.          0.        ]\n",
      " [ 1.2249484  -1.2249484   1.          0.          0.        ]\n",
      " [ 0.16553356 -0.33106712  1.          0.          0.        ]\n",
      " [ 0.19864027 -0.4966007   1.          0.          0.        ]\n",
      " [ 0.19864027 -1.0925215   1.          0.          0.        ]\n",
      " [ 0.06621343 -1.2249484   1.          0.          0.        ]\n",
      " [-0.16553356 -0.43038726  1.          0.          0.        ]\n",
      " [-0.39728054 -0.4966007   1.          0.          0.        ]\n",
      " [-1.5891222  -1.0925215   1.          0.          0.        ]\n",
      " [-0.43038726 -0.03310671  1.          0.          0.        ]\n",
      " [-0.46349397  0.19864027  1.          0.          0.        ]\n",
      " [-0.5959208   0.4966007   1.          0.          0.        ]\n",
      " [-0.16553356  0.26485372  0.          1.          0.        ]\n",
      " [ 0.09932014 -0.62902755  1.          0.          0.        ]\n",
      " [-0.03310671 -0.52970743  1.          0.          0.        ]\n",
      " [ 0.89388126 -1.7877625   0.          1.          0.        ]\n",
      " [-0.5959208   0.92698795  1.          0.          0.        ]\n",
      " [ 0.13242686 -0.16553356  1.          0.          0.        ]\n",
      " [ 0.9932014  -0.2979604   1.          0.          0.        ]\n",
      " [ 0.62902755 -0.03310671  1.          0.          0.        ]\n",
      " [ 1.0263081   0.5959208   1.          0.          0.        ]\n",
      " [-0.26485372  0.          1.          0.          0.        ]\n",
      " [-0.62902755  0.23174699  1.          0.          0.        ]\n",
      " [-0.82766783  0.52970743  1.          0.          0.        ]\n",
      " [-0.72834766  0.03310671  1.          0.          0.        ]\n",
      " [-0.5959208  -0.26485372  0.          1.          0.        ]\n",
      " [ 0.          0.          0.          0.          1.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "44 44\n",
      "27 27\n",
      "34 34\n",
      "49 49\n"
     ]
    }
   ],
   "source": [
    "data = np.load('./data/sketchrnn_apple.npz', encoding='latin1', allow_pickle=True)\n",
    "\n",
    "strokes = StrokesDataset(data['train'], batch_size=4, max_seq_length=200, gpu=False, shuffle=False)\n",
    "print(strokes.data_size)\n",
    "# first item\n",
    "\n",
    "x, t = strokes.__next__()\n",
    "print(x.shape, t.shape) # x is the stroke, t is the mask (x has one more column than t)\n",
    "print(x[0], t[0])\n",
    "\n",
    "# check if the mask is working\n",
    "batch_size = x.shape[0]\n",
    "\n",
    "for i in range(batch_size):\n",
    "    mask_zero_id = np.where(t[i] == 0)[0]\n",
    "    # first id\n",
    "    first_id = mask_zero_id[0]\n",
    "    stroke_end_id = np.where(x[i, :, 4] == 1)[0]\n",
    "    first_stroke_end_id = stroke_end_id[0]\n",
    "    \n",
    "    print(first_id, first_stroke_end_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, d_z, hidden_size):\n",
    "        super().__init__()\n",
    "        self.lstm = L.LSTM(in_size=5, hidden_size=hidden_size)\n",
    "        self.mu_head = L.Linear(in_size=hidden_size, out_size=d_z)\n",
    "        self.sigma_head = L.Linear(in_size=hidden_size, out_size=d_z)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.d_z = d_z\n",
    "\n",
    "    def forward(self, x):\n",
    "        # hidden = self.lstm(x)\n",
    "\n",
    "        # hidden = hidden[:,-1,:]        \n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        h, c = None, None\n",
    "        for i in range(seq_len):\n",
    "            if h is None or c is None:\n",
    "                h, c = self.lstm(x[:, i, :])\n",
    "            else:\n",
    "                h, c = self.lstm(x[:, i, :], h, c)\n",
    "\n",
    "        mu = self.mu_head(h)\n",
    "        sigma_hat = self.sigma_head(h)\n",
    "        sigma = F.exp(sigma_hat / 2.)\n",
    "\n",
    "        xp = dezero.cuda.get_array_module(mu)\n",
    "        z = mu + sigma * xp.random.normal(0, 1, mu.shape)\n",
    "        return z, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, d_z, hidden_size, n_distributions):\n",
    "        super().__init__()\n",
    "        self.lstm = L.LSTM(in_size=d_z+5, hidden_size=hidden_size)\n",
    "\n",
    "        self.init_h = L.Linear(in_size=d_z, out_size=hidden_size)\n",
    "        self.init_c = L.Linear(in_size=d_z, out_size=hidden_size)\n",
    "\n",
    "        self.pi_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.mu_x_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.mu_y_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.sigma_x_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.sigma_y_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.rho_xy_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "\n",
    "        self.q_head = L.Linear(in_size=hidden_size, out_size=3)\n",
    "\n",
    "        self.n_distributions = n_distributions\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x, z, h=None, c=None):\n",
    "        xp = dezero.cuda.get_array_module(x)\n",
    "        h, c = None, None\n",
    "        if h is None and c is None:\n",
    "            h = F.tanh(self.init_h(z))\n",
    "            c = F.tanh(self.init_c(z))\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        outputs = None\n",
    "        for i in range(seq_len):\n",
    "            h, c = self.lstm(x[:, i, :], h, c)\n",
    "            if outputs == None:\n",
    "                outputs = F.expand_dims(h, 1)\n",
    "            else:\n",
    "                outputs = F.cat((outputs, F.expand_dims(h, 1)), axis=1)\n",
    "\n",
    "        # hidden Needs to chagned to output of lstm\n",
    "        # print(outputs.shape)\n",
    "        # print(self.q_head(outputs).shape)\n",
    "\n",
    "        outputs= F.reshape(outputs, (-1, self.hidden_size))\n",
    "        q_logits = F.log_softmax(self.q_head(outputs))\n",
    "        # print(q_logits.shape, \"q_logits\")\n",
    "\n",
    "        pi_logits = self.pi_head(outputs)\n",
    "        mu_x = self.mu_x_head(outputs)\n",
    "        mu_y = self.mu_y_head(outputs)\n",
    "        sigma_x = self.sigma_x_head(outputs)\n",
    "        sigma_y = self.sigma_y_head(outputs)\n",
    "        rho_xy = self.rho_xy_head(outputs)\n",
    "\n",
    "        pi_logits = F.reshape(pi_logits, (-1, seq_len, self.n_distributions))\n",
    "        mu_x = F.reshape(mu_x, (-1, seq_len, self.n_distributions))\n",
    "        mu_y = F.reshape(mu_y, (-1, seq_len, self.n_distributions))\n",
    "        sigma_x = F.reshape(sigma_x, (-1, seq_len, self.n_distributions))\n",
    "        sigma_y = F.reshape(sigma_y, (-1, seq_len, self.n_distributions))\n",
    "        rho_xy = F.reshape(rho_xy, (-1, seq_len, self.n_distributions))\n",
    "        \n",
    "        q_logits = F.reshape(q_logits, (-1, seq_len, 3))\n",
    "\n",
    "\n",
    "        bgm = BivariateGaussianMixture(pi_logits, mu_x, mu_y, F.exp(sigma_x), F.exp(sigma_y), F.tanh(rho_xy))\n",
    "        return bgm, q_logits, h, c\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ReconstructionLoss(target, mask, bgm, q_logits):\n",
    "#     xp = cuda.get_array_module(mask)\n",
    "#     xy = target[:, :, 0:2]\n",
    "#     xy = xy[:, :, xp.newaxis, :]\n",
    "#     xy = xp.tile(xy, (1, 1, bgm.n_distributions, 1))\n",
    "    \n",
    "#     # expanded_shape = (xy.shape[0], xy.shape[1], bgm.n_distributions, xy.shape[3])\n",
    "    \n",
    "#     x = xy[:,:,:,0]\n",
    "#     y = xy[:,:,:,0]\n",
    "    \n",
    "#     loss_stroke = F.mul(bgm.get_lossfunc(x, y), mask)\n",
    "    \n",
    "#     loss_pen = -F.mean(F.mul(target[:,:,2:], q_logits))\n",
    "    \n",
    "#     return F.add(loss_stroke, loss_pen)\n",
    "\n",
    "\n",
    "def ReconstructionLoss(mask, target, bgm, q_logits):\n",
    "        xp = cuda.get_array_module(mask)\n",
    "        # target is a 3 dimensional array\n",
    "        # xy = target[:, :, 0:2].unsqueeze(-2).expand(-1, -1, dist.n_distributions, -1)\n",
    "        xy = target[:, :, 0:2]\n",
    "        x = xy[:, :, 0]\n",
    "        y = xy[:, :, 1]\n",
    "        \n",
    "        distributions = bgm.n_distributions\n",
    "        stacked_x = None\n",
    "        stacked_y = None\n",
    "        for i in range(distributions):\n",
    "            if stacked_x is None:\n",
    "                stacked_x = F.expand_dims(x, axis=2)\n",
    "                stacked_y = F.expand_dims(y, axis=2)\n",
    "            else:\n",
    "                stacked_x = F.cat((stacked_x, F.expand_dims(x, axis=2)), axis=2)\n",
    "                stacked_y = F.cat((stacked_y, F.expand_dims(y, axis=2)), axis=2)\n",
    "        \n",
    "        # expanded_shape = (xy.shape[0], xy.shape[1], bgm.n_distributions, xy.shape[3])\n",
    "        \n",
    "        \n",
    "        # x = xp.tile(xy, expanded_shape)\n",
    "        # y = xp.tile(xy, expanded_shape)\n",
    "        # loss_stroke 에 문제\n",
    "        \n",
    "        loss_stroke = bgm.get_lossfunc(stacked_x, stacked_y, mask)\n",
    "        \n",
    "        loss_pen = -F.mean(F.mul(target[:,:,2:], q_logits))\n",
    "        \n",
    "        return F.add(loss_stroke, loss_pen)\n",
    "        \n",
    "        \n",
    "\n",
    "def KLDivergenceLoss(mu, sigma):\n",
    "    xp = cuda.get_array_module(mu)\n",
    "    tmp = xp.ones(sigma.shape)\n",
    "    inner_1 = F.add(tmp, sigma)\n",
    "    inner_2 = F.add(F.pow(mu, 2), F.exp(sigma))\n",
    "    inner = F.sub(inner_1, inner_2)\n",
    "    tmp2 = xp.ones(inner.shape) * -2\n",
    "    return F.mean(F.div(inner, tmp2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, d_z, enc_hidden_size, dec_hidden_size, n_distributions):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_z, enc_hidden_size)\n",
    "        self.decoder = Decoder(d_z, dec_hidden_size, n_distributions)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        z, mu, sigma = self.encoder(x)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        xp = cuda.get_array_module(z)\n",
    "\n",
    "        expanded_z = F.expand_dims(z, axis=1)\n",
    "        \n",
    "        z_stack = None\n",
    "        \n",
    "        for i in range(seq_len - 1):\n",
    "            if i == 0:\n",
    "                z_stack = expanded_z\n",
    "            else:\n",
    "                z_stack = F.cat((z_stack, expanded_z), axis=1)\n",
    "        # x = F.expand_dims(z, axis=1)\n",
    "\n",
    "        inputs = F.cat((x[:,:-1], z_stack), axis=2)\n",
    "        # inputs = dezero.as_variable(inputs)\n",
    "        bgm, q_logits, _, _ = self.decoder(inputs, z)\n",
    "\n",
    "        kl_loss = KLDivergenceLoss(mu, sigma)\n",
    "\n",
    "\n",
    "        rec_loss = ReconstructionLoss(t, x[:,1:], bgm, q_logits)\n",
    "        # \n",
    "        return kl_loss + rec_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_z = 4\n",
    "enc_hidden_size = 32\n",
    "dec_hidden_size = 64\n",
    "n_distributions = 8\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.861282041980405\n",
      "5.127715155637381\n",
      "4.629630106136995\n",
      "5.457939746395556\n",
      "4.324532361895657\n",
      "3.548348329179838\n",
      "3.774540370112671\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m model(x, t)\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mcleargrads()\n\u001b[0;32m---> 11\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/Github/Sketch-RNN/dezero/core.py:141\u001b[0m, in \u001b[0;36mVariable.backward\u001b[0;34m(self, retain_grad, create_graph)\u001b[0m\n\u001b[1;32m    138\u001b[0m             x\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m gx\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcreator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m             \u001b[43madd_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m retain_grad:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39moutputs:\n",
      "File \u001b[0;32m~/Github/Sketch-RNN/dezero/core.py:122\u001b[0m, in \u001b[0;36mVariable.backward.<locals>.add_func\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    120\u001b[0m funcs\u001b[38;5;241m.\u001b[39mappend(f)\n\u001b[1;32m    121\u001b[0m seen_set\u001b[38;5;241m.\u001b[39madd(f)\n\u001b[0;32m--> 122\u001b[0m \u001b[43mfuncs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/Sketch-RNN/dezero/core.py:122\u001b[0m, in \u001b[0;36mVariable.backward.<locals>.add_func.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    120\u001b[0m funcs\u001b[38;5;241m.\u001b[39mappend(f)\n\u001b[1;32m    121\u001b[0m seen_set\u001b[38;5;241m.\u001b[39madd(f)\n\u001b[0;32m--> 122\u001b[0m funcs\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mgeneration)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dezero.optimizers import Adam\n",
    "\n",
    "model = VAE(d_z, enc_hidden_size, dec_hidden_size, n_distributions)\n",
    "optimizer = Adam().setup(model)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(strokes.max_iter):\n",
    "        x, t = strokes.__next__()\n",
    "        loss = model(x, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        print(loss.data)\n",
    "        \n",
    "    print(f\"Epoch {epoch} finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    def __init__(self, encoder, decoder):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def sample(self, x, temperature=1.0):\n",
    "        # x is a batch of stroke data\n",
    "        longest_seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        z, _, _ = self.encoder(x)\n",
    "        xp = cuda.get_array_module(x)\n",
    "        \n",
    "        s = xp.zeros(5)\n",
    "        # mark the start of the sequence\n",
    "        s[2] = 1\n",
    "        \n",
    "        print(s)\n",
    "        # s is the initial stroke\n",
    "        # we are going to create a sequence of strokes\n",
    "        # we first \"\"\n",
    "        seq = xp.array([s])\n",
    "        \n",
    "        h = None\n",
    "        c = None\n",
    "        \n",
    "        \n",
    "        with dezero.no_grad():\n",
    "            for i in range(longest_seq_len):\n",
    "                expanded_z = F.expand_dims(z, axis=1)\n",
    "                # change s to a 3D array to have (1,1,)\n",
    "                \n",
    "                morphed_s = F.reshape(s, (1, 1, -1))\n",
    "                \n",
    "                stacked_s = None\n",
    "                for i in range(batch_size):\n",
    "                    if stacked_s is None:\n",
    "                        stacked_s = morphed_s\n",
    "                    else:\n",
    "                        stacked_s = F.cat((stacked_s, morphed_s), axis=0)\n",
    "                        \n",
    "                \n",
    "                # print(stacked_s.shape, expanded_z.shape, \"stacked_s, expanded_z\")\n",
    "                inputs = F.cat((stacked_s, expanded_z), axis=2)\n",
    "                # 여기 아래 부분에서 오류가 생긴다\n",
    "                \n",
    "                if h is None and c is None:\n",
    "                    bgm, q_logits, h, c = self.decoder(inputs, z)\n",
    "                else:\n",
    "                    bgm, q_logits, h, c = self.decoder(inputs, z, h, c)\n",
    "                    \n",
    "                \n",
    "                \n",
    "                s = self._sample_step(bgm, q_logits, temperature)\n",
    "                \n",
    "                xp.append(seq, s)\n",
    "                # seq.append(s)\n",
    "                \n",
    "                if s[4] == 1:\n",
    "                    break\n",
    "                \n",
    "        seq = F.cat(seq, axis=1)\n",
    "        \n",
    "        return seq\n",
    "\n",
    "                \n",
    "    def _sample_step(self, bgm, q_logits, temperature):\n",
    "        xp = cuda.get_array_module(x)\n",
    "        bgm.set_temperature(temperature)\n",
    "        \n",
    "        # pring if bgm pi sum is 1\n",
    "        \n",
    "        # print(bgm.pi_logits.shape, 'pi logits shape in sample step')\n",
    "        seq_len = bgm.pi_logits.shape[1]\n",
    "        # print(F.reshape(F.sum(bgm.pi_logits, axis=2), (-1, seq_len)), 'sum of pi _sample_step')\n",
    "        gen_result = bgm.sample(1)\n",
    "        \n",
    "        # we need categorical distribution q for pen state\n",
    "        # convert scaled logits to probabilities\n",
    "        probabilities = F.softmax(q_logits / temperature, axis=-1)\n",
    "        \n",
    "        gen_x = gen_result[:, :, 0]\n",
    "        gen_y = gen_result[:, :, 1]\n",
    "        \n",
    "        # sample from categorical distribution\n",
    "        # we need to sample from the pen state\n",
    "        xp = cuda.get_array_module(gen_x)\n",
    "        stroke = xp.zeros(5, dtype=q_logits.dtype)\n",
    "        \n",
    "        \n",
    "        print(gen_x.shape, gen_y.shape, probabilities, \"gen_x, gen_y, probabilities\")\n",
    "        # print(stroke, \"stroke\")\n",
    "        # fill in gen_x and gen_y\n",
    "        # print(gen_x[0][0], gen_y[0][0])\n",
    "        stroke[0] = gen_x[0][0]\n",
    "        stroke[1] = gen_y[0][0]\n",
    "        \n",
    "        # sample from the categorical distribution\n",
    "        # we need to sample from the pen state\n",
    "        print(probabilities[0][0].data, 'probabilities')\n",
    "        # choose pen state from 0, 1, 2\n",
    "        pen = xp.random.choice(3, 1, p=probabilities[0][0].data)\n",
    "        print(pen)\n",
    "        \n",
    "        stroke[2 + pen] = 1\n",
    "        \n",
    "        return stroke\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  (4, 64, 5)\n",
      "[0. 0. 1. 0. 0.]\n",
      "(1, 15) (1, 15) variable([[[0.33333333 0.33333333 0.33333333]]\n",
      "         \n",
      "          [[0.33333333 0.33333333 0.33333333]]\n",
      "         \n",
      "          [[0.33333333 0.33333333 0.33333333]]\n",
      "         \n",
      "          [[0.33333333 0.33333333 0.33333333]]]) gen_x, gen_y, probabilities\n",
      "[0.33333333 0.33333333 0.33333333] probabilities\n",
      "[1]\n",
      "(1, 15) (1, 15) variable([[[0.33333333 0.33333333 0.33333333]]\n",
      "         \n",
      "          [[0.33333333 0.33333333 0.33333333]]\n",
      "         \n",
      "          [[0.33333333 0.33333333 0.33333333]]\n",
      "         \n",
      "          [[0.33333333 0.33333333 0.33333333]]]) gen_x, gen_y, probabilities\n",
      "[0.33333333 0.33333333 0.33333333] probabilities\n",
      "[0]\n",
      "(1, 15) (1, 15) variable([[[0.33333333 0.33333333 0.33333333]]\n",
      "         \n",
      "          [[0.33333333 0.33333333 0.33333333]]\n",
      "         \n",
      "          [[0.33333333 0.33333333 0.33333333]]\n",
      "         \n",
      "          [[0.33333333 0.33333333 0.33333333]]]) gen_x, gen_y, probabilities\n",
      "[0.33333333 0.33333333 0.33333333] probabilities\n",
      "[2]\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# add dimension to make it compatible with the model, batch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m,batch_data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m# we have one batch\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43msampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 62\u001b[0m, in \u001b[0;36mSampler.sample\u001b[0;34m(self, x, temperature)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m s[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     60\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m seq \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m seq\n",
      "File \u001b[0;32m~/Github/Sketch-RNN/dezero/functions.py:137\u001b[0m, in \u001b[0;36mcat\u001b[0;34m(inputs, axis)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcat\u001b[39m(inputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCat\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/Sketch-RNN/dezero/core.py:207\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    204\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [as_variable(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m    206\u001b[0m xs \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m--> 207\u001b[0m ys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ys, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    209\u001b[0m     ys \u001b[38;5;241m=\u001b[39m (ys,)\n",
      "File \u001b[0;32m~/Github/Sketch-RNN/dezero/functions.py:115\u001b[0m, in \u001b[0;36mCat.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs):\n\u001b[1;32m    114\u001b[0m     xp \u001b[38;5;241m=\u001b[39m cuda\u001b[38;5;241m.\u001b[39mget_array_module(inputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 115\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "sampler = Sampler(model.encoder, model.decoder)\n",
    "\n",
    "batch_data = x[0:batch_size] # since we trained with batch size of 4, we can only sample 4 sketches\n",
    "# add dimension to make it compatible with the model, batch\n",
    "print(\"input shape: \",batch_data.shape) # we have one batch\n",
    "\n",
    "sample = sampler.sample(batch_data, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
